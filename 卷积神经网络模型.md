---
title: 卷积神经网络模型
date: 2021-12-04 03:08:51
tags: Pytorch
---

### 卷积神经网络（LeNet)

![Screen Shot 2021-12-05 at 12.25.06 pm](/Users/kevin/Library/Application Support/typora-user-images/Screen Shot 2021-12-05 at 12.25.06 pm.jpg)

**模型结构**：卷积层块， 全链接层块

* 卷积层块：2个**卷积层 + 最大池化层** 的结构组成。 由于LeNet是较早的CNN， 在每个卷积层 + 池化层后多会跟一个sigmod层 来修正输出结果。 而现在用的较多的是Relu。
* 全连接层块：输入为二维向量。 单卷积层块的输出传入全连接层的时候会对小批量对每个样本进行扁平化（flatten）

LeNet 会随着网络的加深，宽度逐渐降低，通道逐渐增多。

### 深度卷积神经网络（AlexNet）

### ![Screen Shot 2021-12-05 at 12.24.03 pm](/Users/kevin/Library/Application Support/typora-user-images/Screen Shot 2021-12-05 at 12.24.03 pm.jpg)

**模型结构**：5层卷积 + 2层全连接隐藏层 + 1层全连接输出层

* 卷积层： 前2个用的分别是11x11和5x5的卷积核，其余的都是3x3的卷积核。 第一， 第二， 第五个卷积层后都使用了3x3，步幅为2 的最大池化层。
* 全连接层：2个输出个数为4096的全连接层携带着将近1GB的模型参数。 
* 激活函数：AlexNet使用了Relu激活函数。相比于sigmod，Relu有着更简单的计算并且在不同初始化的情况下更容易训练。例如在一些特殊初始化下， sigmod在正区间的输出极度接近0， 这会导致模型很难继续更新，而Relu在正区间的值恒为1。
* 过拟合：AlexNet使用了丢弃法来控制模型复杂度和防止过拟合。并且它用了大量的图象增广， 包括翻转， 裁剪， 改变颜色等，来进一步防止过拟合。

### 使用重复元素的网络（VGG）

<img src="/Users/kevin/Library/Application Support/typora-user-images/Screen Shot 2021-12-05 at 12.29.11 pm.jpg" alt="Screen Shot 2021-12-05 at 12.29.11 pm" style="zoom:50%;" />

**模型结构**：VGG块 + 全连接层块

* VGG块：卷积层 + 池化层， 卷积层都是用相通的填充为1，3x3的卷积核接上一个步幅为2 ， 窗口为2x2的最大池化层
* 全连接层块：与LeNet相似

VGG是个十分对称的网络，每层都成倍的增加或者减少。相比AlexNet它提供了一种简单固定的卷积模型和深度模型的构建思路。

### 网络中的网络（NiN）

![Screen Shot 2021-12-05 at 12.44.22 pm](/Users/kevin/Library/Application Support/typora-user-images/Screen Shot 2021-12-05 at 12.44.22 pm.jpg)

**模型结构** ： NiN块

* NiN块：AlexNet是用多个卷积层 + 全连接层输出的结构，NiN提出了另一种思路， 它通过将小块**卷积层+“全连接”层**串联组成网络。由于全连接层是二维而卷积层通常来说是四维的， 所以NiN块使用1x1的卷积层代替全连接层（期中空间维度（高宽）上的每一个元素相当于样本，通道相当于特征）。每个卷积层与AlexNet类似，都是11x11， 5x5， 3x3.并且每个NiN块后接一个步幅为2，窗口大小为3x3的最大池化层。 

相比AlexNet，NiN去掉了最后3个全连接层，使用输出通道等于标签类别的NiN块， 然后使用全局平局池化层对每个通道中所有元素求平均并直接用于分类。 这个好处是可以显著减小模型参数尺寸， 但会造成训练时间的增加。 

### 含有并行连接的网络（GoogLeNet）

![img](https://pytorch.org/assets/images/googlenet1.png)

* Inception块：GoogLeNet的基础块，它借鉴NiN的网络串联网络的思路。 在每个Inception块中包含4条并行线路。前3条分别使用1x1， 3x3， 5x5的卷积层来抽取不通空间尺度下的特征信息， 期中第二三条线中先使用了1x1的卷积层来减少输入通道数，以降低模型复杂度。 最后一条使用3x3的最大池化层接1x1的卷积层来改变通道数。4条线都适用合适的填充来保证输入和输出的高宽一致。 

![img](https://miro.medium.com/max/2542/1*rXcdL9OV5YKlYyks9XK-wA.png)

### 残差网络（ResNet)

![Screen Shot 2021-12-05 at 2.37.01 pm](/Users/kevin/Library/Application Support/typora-user-images/Screen Shot 2021-12-05 at 2.37.01 pm.jpg)

* 残差块：一般来说对于激活函数的输入是神经网络一层层的计算的输出结果，但是由于网络的不断加深容易出现梯度不稳定（梯度爆炸，梯度消失）。随着网络的逐渐加深，误差并不会越来越小残差块的目的就是为了解决梯度不稳定。 它通过一种跳跃的连接方式让输出结果需要参考输入结果。	

![img](https://miro.medium.com/max/874/1*R-Yzqn6VLmIyITO3ZxA1sQ.png)

* 残差块原理：$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})=g(w^{[l+2]}a^{[l+1]} + b^{[l+2]}a^{[l]})$ 我们现在不考虑 $b^{[l+2]}$, 当发生梯度消失的时候， $w^{[l+2]}=0$, 此时$a^{[l+2]}=g(a^{[l]})$, 相当于把第一层的输出直接经过Relu输出。并不会因为梯度消失产生负面的影响。 



### 稠密连接网络（DenseNet）

<img src="https://pytorch.org/assets/images/densenet1.png" alt="img" style="zoom:60%;" />

**模型结构**：稠密层 + 过渡层

* 稠密层：DenseNet和ResNet十分相似，区别在于DenseNet不像ResNet将前一个模块的输出直接加到模块的输出上而是直接在通道上进行叠加
* 过渡层：为了防止通道数一直叠加导致模型复杂度过大，过渡层通过使用1x1的卷积层减小通道数，并使用步幅为2的平均池化层减半高宽进一步降低复杂度。 







 
